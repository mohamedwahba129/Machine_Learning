{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fc8f92ec580f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import string\n",
    "import unicodedata\n",
    "import collections\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>This soundtrack is my favorite music of all ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack</td>\n",
       "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>If you've played the game, you know how divine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>an absolute masterpiece</td>\n",
       "      <td>I am quite sure any of you actually taking the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                              title  \\\n",
       "0         2              The best soundtrack ever to anything.   \n",
       "1         2                                           Amazing!   \n",
       "2         2                               Excellent Soundtrack   \n",
       "3         2  Remember, Pull Your Jaw Off The Floor After He...   \n",
       "4         2                            an absolute masterpiece   \n",
       "\n",
       "                                                text  \n",
       "0  I'm reading a lot of reviews saying that this ...  \n",
       "1  This soundtrack is my favorite music of all ti...  \n",
       "2  I truly like this soundtrack and I enjoy video...  \n",
       "3  If you've played the game, you know how divine...  \n",
       "4  I am quite sure any of you actually taking the...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train['text']\n",
    "text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words('english')\n",
    "seed = 78\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of numerical features : \n",
      "            polarity\n",
      "count  1.048575e+06\n",
      "mean   1.505600e+00\n",
      "std    4.999689e-01\n",
      "min    1.000000e+00\n",
      "25%    1.000000e+00\n",
      "50%    2.000000e+00\n",
      "75%    2.000000e+00\n",
      "max    2.000000e+00\n",
      "\n",
      "Total number of reviews:  1048575\n",
      "\n",
      "Percentage of reviews with neutral sentiment : 0.00%\n",
      "\n",
      "Percentage of reviews with positive sentiment : 49.44%\n",
      "\n",
      "Percentage of reviews with negative sentiment : 50.56%\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary statistics of numerical features : \\n\", train.describe())\n",
    "\n",
    "print(\"\\nTotal number of reviews: \",len(train))\n",
    "\n",
    "print(\"\\nPercentage of reviews with neutral sentiment : {:.2f}%\"\\\n",
    "      .format(train[train['polarity']==3][\"text\"].count()/len(train)*100))\n",
    "print(\"\\nPercentage of reviews with positive sentiment : {:.2f}%\"\\\n",
    "      .format(train[train['polarity']==1][\"text\"].count()/len(train)*100))\n",
    "print(\"\\nPercentage of reviews with negative sentiment : {:.2f}%\"\\\n",
    "      .format(train[train['polarity']==2][\"text\"].count()/len(train)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot distribution of rating\n",
    "plt.figure(figsize=(12,8))\n",
    "# sns.countplot(df['Rating'])\n",
    "train['polarity'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>875680</th>\n",
       "      <td>2</td>\n",
       "      <td>We LOVE Night Catch!!</td>\n",
       "      <td>We bought this book so I could video tape my h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046906</th>\n",
       "      <td>1</td>\n",
       "      <td>RIDICULOUSLY priced!</td>\n",
       "      <td>You can get it for $24.95 + tax from ULC, and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646861</th>\n",
       "      <td>1</td>\n",
       "      <td>Waste of money and did not last 2 years! RUNNN...</td>\n",
       "      <td>I bought this printer less than two years ago....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704385</th>\n",
       "      <td>1</td>\n",
       "      <td>Not the best</td>\n",
       "      <td>After reading 13 sequels to POTO this is one o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798051</th>\n",
       "      <td>1</td>\n",
       "      <td>Huh? Who? How? What Happened?</td>\n",
       "      <td>This movie could have gone down in history as ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity                                              title  \\\n",
       "875680          2                              We LOVE Night Catch!!   \n",
       "1046906         1                               RIDICULOUSLY priced!   \n",
       "646861          1  Waste of money and did not last 2 years! RUNNN...   \n",
       "704385          1                                       Not the best   \n",
       "798051          1                      Huh? Who? How? What Happened?   \n",
       "\n",
       "                                                      text  Sentiment  \n",
       "875680   We bought this book so I could video tape my h...          1  \n",
       "1046906  You can get it for $24.95 + tax from ULC, and ...          0  \n",
       "646861   I bought this printer less than two years ago....          0  \n",
       "704385   After reading 13 sequels to POTO this is one o...          0  \n",
       "798051   This movie could have gone down in history as ...          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.sample(frac=0.1, random_state=0) #uncomment to use full set of data\n",
    "\n",
    "# Drop missing values\n",
    "#train.dropna(inplace=True)\n",
    "\n",
    "# Encode 2s as 1 (positive sentiment) and 1s as 0 (negative sentiment)\n",
    "train['Sentiment'] = np.where(train['polarity']==1, 0, 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 78643 training examples and 26215 validation examples. \n",
      "\n",
      "Show a review in the training set : \n",
      " I have an old LP album of the same title as this Cd and with the same photo of Fats on the cover that I consider excellant. I thought this Cd was that album but its not- not even close. Very disapointing.\n"
     ]
    }
   ],
   "source": [
    "# Split data into training set and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.text, train.Sentiment, random_state=0)\n",
    "#train_test_split(train['text'], train['Sentiment'], test_size=0.25, random_state=0)\n",
    "\n",
    "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n",
    "print('Show a review in the training set : \\n', X_train.iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case \n",
    "    \n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True: # stemming\n",
    "#         stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "    \n",
    "    return( \" \".join(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show a cleaned review in the training set : \n",
      " i have an old lp album of the same title as this cd and with the same photo of fats on the cover that i consider excellant i thought this cd was that album but its not not even close very disapointing\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text data in training set and validation set\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
    "    \n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78643 parsed sentence in the training set\n",
      "\n",
      "Show a parsed sentence in the training set : \n",
      " ['i', 'have', 'an', 'old', 'lp', 'album', 'of', 'the', 'same', 'title', 'as', 'this', 'cd', 'and', 'with', 'the', 'same', 'photo', 'of', 'fats', 'on', 'the', 'cover', 'that', 'i', 'consider', 'excellant', 'i', 'thought', 'this', 'cd', 'was', 'that', 'album', 'but', 'its', 'not', 'not', 'even', 'close', 'very', 'disapointing']\n"
     ]
    }
   ],
   "source": [
    "# Split review text into parsed sentences uisng NLTK's punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Parse text into sentences\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "    \n",
    "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
    "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model ...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-49c7661c2637>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Word2Vec model ...\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m w2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,\\\n\u001b[0m\u001b[0;32m     12\u001b[0m                  window = context, sample = downsampling)\n\u001b[0;32m     13\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit parsed sentences to Word2Vec model \n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "num_features = 300  #embedding dimension                     \n",
    "min_word_count = 10                \n",
    "num_workers = 4       \n",
    "context = 10                                                                                          \n",
    "downsampling = 1e-3 \n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,\\\n",
    "                 window = context, sample = downsampling)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n",
    "\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) #4016 \n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom the training data into feature vectors\n",
    "\n",
    "def makeFeatureVec(review, model, num_features):\n",
    "    '''\n",
    "    Transform a review to a feature vector by averaging feature vectors of words \n",
    "    appeared in that review and in the volcabulary list created\n",
    "    '''\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word) #index2word is the volcabulary list of the Word2Vec model\n",
    "    isZeroVec = True\n",
    "    for word in review:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "            isZeroVec = False\n",
    "    if isZeroVec == False:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    '''\n",
    "    Transform all reviews to feature vectors using makeFeatureVec()\n",
    "    '''\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature vectors for training set\n",
    "X_train_cleaned = []\n",
    "for review in X_train:\n",
    "    X_train_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "trainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\n",
    "print(\"Training set : %d feature vectors with %d dimensions\" %trainVector.shape)\n",
    "\n",
    "\n",
    "# Get feature vectors for validation set\n",
    "X_test_cleaned = []\n",
    "for review in X_test:\n",
    "    X_test_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "testVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\n",
    "print(\"Validation set : %d feature vectors with %d dimensions\" %testVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 20000 \n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "nb_epoch = 3\n",
    "\n",
    "\n",
    "# Vectorize X_train and X_test to 2D tensor\n",
    "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# one-hot encoding of y_train and y_test\n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq.shape)\n",
    "print('X_test shape:', X_test_seq.shape)\n",
    "print('y_train shape:', y_train_seq.shape)\n",
    "print('y_test shape:', y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a simple LSTM\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, 128, dropout=0.2))\n",
    "model1.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.summary()\n",
    "\n",
    "# Compile LSTM\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
    "\n",
    "# Model evluation\n",
    "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weight matrix of the embedding layer\n",
    "model1.layers[0].get_weights()[0] # weight matrix of the embedding layer, word-by-dim matrix\n",
    "print(\"Size of weight matrix in the embedding layer : \", \\\n",
    "      model1.layers[0].get_weights()[0].shape)\n",
    "\n",
    "# get weight matrix of the hidden layer\n",
    "print(\"Size of weight matrix in the hidden layer : \", \\\n",
    "      model1.layers[1].get_weights()[0].shape) # weight dim of LSTM - w\n",
    "\n",
    "# get weight matrix of the output layer\n",
    "print(\"Size of weight matrix in the output layer : \", \\\n",
    "      model1.layers[2].get_weights()[0].shape) # weight dim of dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained Word2Vec model\n",
    "w2v = Word2Vec.load(\"w2v_300features_10minwordcounts_10context\")\n",
    "\n",
    "\n",
    "# Get Word2Vec embedding matrix\n",
    "embedding_matrix = w2v.wv.syn0  # embedding matrix, type = numpy.ndarray \n",
    "print(\"Shape of embedding matrix : \", embedding_matrix.shape) #(volcabulary size, embedding dimension)\n",
    "# w2v.wv.syn0[0] #feature vector of the first word in the volcabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = embedding_matrix.shape[0] #4016\n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "nb_epoch = 3\n",
    "\n",
    "\n",
    "# Vectorize X_train and X_test to 2D tensor\n",
    "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# one-hot encoding of y_train and y_test\n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq.shape)\n",
    "print('X_test shape:', X_test_seq.shape)\n",
    "print('y_train shape:', y_train_seq.shape)\n",
    "print('y_test shape:', y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Word2Vec embedding layer\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], #4016\n",
    "                            embedding_matrix.shape[1], #300\n",
    "                            weights=[embedding_matrix])\n",
    "\n",
    "\n",
    "# Construct LSTM with Word2Vec embedding\n",
    "model2 = Sequential()\n",
    "model2.add(embedding_layer)\n",
    "model2.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model2.add(Dense(nb_classes))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()\n",
    "\n",
    "# Compile model\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "score = model2.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weight matrix of the embedding layer\n",
    "print(\"Size of weight matrix in the embedding layer : \", \\\n",
    "      model2.layers[0].get_weights()[0].shape)\n",
    "\n",
    "# get weight matrix of the hidden layer\n",
    "print(\"Size of weight matrix in the hidden layer : \", \\\n",
    "      model2.layers[1].get_weights()[0].shape) # weight dim of LSTM - w\n",
    "\n",
    "# get weight matrix of the output layer\n",
    "print(\"Size of weight matrix in the output layer : \", \\\n",
    "      model2.layers[2].get_weights()[0].shape) # weight dim of dense layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
