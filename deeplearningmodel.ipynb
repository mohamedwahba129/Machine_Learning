{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearningmodel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnEn3A8lQxJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae8362b-7496-4bd1-ad3c-1d84a19530f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIrkHV49laQb",
        "outputId": "fdba445e-03d5-4fc7-acbf-f13f605f274c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.0.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=33dc132ed55f2ef9a0cb97790f38a9eb592699adb75fd8a1bb8ffe9d70dfe4eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tqdm\n",
        "import string\n",
        "import unicodedata\n",
        "import collections\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from bs4 import BeautifulSoup  \n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import logging\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras import backend as K\n",
        "from keras.layers.embeddings import Embedding"
      ],
      "metadata": {
        "id": "ofjgom3QrRXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stop_words = stopwords.words('english')\n",
        "seed = 78\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = RegexpTokenizer(r'[a-z]+')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM4bu2uLkPax",
        "outputId": "94a14047-ca73-40d2-9f92-694a54fa1466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/amazon reviews/train.csv')\n",
        "train.head()\n",
        "#print(f'Train shape: {train.shape}')\n",
        "#- Test shape: {test.shape}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pHGHYN-0lBGv",
        "outputId": "3bdf678a-d7c0-4628-b537-0b84e5df7525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5e74dd8a-ee34-440d-b7e9-413d7246eb29\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>The best soundtrack ever to anything.</td>\n",
              "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Amazing!</td>\n",
              "      <td>This soundtrack is my favorite music of all ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Excellent Soundtrack</td>\n",
              "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
              "      <td>If you've played the game, you know how divine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>an absolute masterpiece</td>\n",
              "      <td>I am quite sure any of you actually taking the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e74dd8a-ee34-440d-b7e9-413d7246eb29')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5e74dd8a-ee34-440d-b7e9-413d7246eb29 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5e74dd8a-ee34-440d-b7e9-413d7246eb29');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   polarity  ...                                               text\n",
              "0         2  ...  I'm reading a lot of reviews saying that this ...\n",
              "1         2  ...  This soundtrack is my favorite music of all ti...\n",
              "2         2  ...  I truly like this soundtrack and I enjoy video...\n",
              "3         2  ...  If you've played the game, you know how divine...\n",
              "4         2  ...  I am quite sure any of you actually taking the...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = train['text']\n",
        "text.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_KtIZyNpt9H",
        "outputId": "ccc4d425-cb77-4e2e-b544-2f07d9cc9333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1048575"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "_GpXxSXvtCng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary statistics of numerical features : \\n\", train.describe())\n",
        "\n",
        "print(\"\\nTotal number of reviews: \",len(train))\n",
        "\n",
        "print(\"\\nPercentage of reviews with neutral sentiment : {:.2f}%\"\\\n",
        "      .format(train[train['polarity']==3][\"text\"].count()/len(train)*100))\n",
        "print(\"\\nPercentage of reviews with positive sentiment : {:.2f}%\"\\\n",
        "      .format(train[train['polarity']==1][\"text\"].count()/len(train)*100))\n",
        "print(\"\\nPercentage of reviews with negative sentiment : {:.2f}%\"\\\n",
        "      .format(train[train['polarity']==2][\"text\"].count()/len(train)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwvYZCY8s8Q8",
        "outputId": "68e00d4f-1e7f-48ea-f455-e28644816adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics of numerical features : \n",
            "            polarity\n",
            "count  1.048575e+06\n",
            "mean   1.505600e+00\n",
            "std    4.999689e-01\n",
            "min    1.000000e+00\n",
            "25%    1.000000e+00\n",
            "50%    2.000000e+00\n",
            "75%    2.000000e+00\n",
            "max    2.000000e+00\n",
            "\n",
            "Total number of reviews:  1048575\n",
            "\n",
            "Percentage of reviews with neutral sentiment : 0.00%\n",
            "\n",
            "Percentage of reviews with positive sentiment : 49.44%\n",
            "\n",
            "Percentage of reviews with negative sentiment : 50.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization"
      ],
      "metadata": {
        "id": "y9HSfdbgu6WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot distribution of rating\n",
        "plt.figure(figsize=(12,8))\n",
        "# sns.countplot(df['Rating'])\n",
        "train['polarity'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Distribution of Rating')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "ETymlJZVu2r0",
        "outputId": "6beadee8-2855-494f-fe2f-fc80bade9785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Count')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "8P5Z83dzv932"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.sample(frac=0.1, random_state=0) #uncomment to use full set of data\n",
        "\n",
        "# Drop missing values\n",
        "#train.dropna(inplace=True)\n",
        "\n",
        "# Encode 2s as 1 (positive sentiment) and 1s as 0 (negative sentiment)\n",
        "train['Sentiment'] = np.where(train['polarity']==1, 0, 1)\n",
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "J9h8Xn7wvXOl",
        "outputId": "bc1ac0a3-c03d-43f4-c95b-c61ccc34bca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0cca4619-aea1-4f1a-9ab0-99d6faa15ce2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>875680</th>\n",
              "      <td>2</td>\n",
              "      <td>We LOVE Night Catch!!</td>\n",
              "      <td>We bought this book so I could video tape my h...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1046906</th>\n",
              "      <td>1</td>\n",
              "      <td>RIDICULOUSLY priced!</td>\n",
              "      <td>You can get it for $24.95 + tax from ULC, and ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>646861</th>\n",
              "      <td>1</td>\n",
              "      <td>Waste of money and did not last 2 years! RUNNN...</td>\n",
              "      <td>I bought this printer less than two years ago....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>704385</th>\n",
              "      <td>1</td>\n",
              "      <td>Not the best</td>\n",
              "      <td>After reading 13 sequels to POTO this is one o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798051</th>\n",
              "      <td>1</td>\n",
              "      <td>Huh? Who? How? What Happened?</td>\n",
              "      <td>This movie could have gone down in history as ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cca4619-aea1-4f1a-9ab0-99d6faa15ce2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0cca4619-aea1-4f1a-9ab0-99d6faa15ce2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0cca4619-aea1-4f1a-9ab0-99d6faa15ce2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         polarity  ... Sentiment\n",
              "875680          2  ...         1\n",
              "1046906         1  ...         0\n",
              "646861          1  ...         0\n",
              "704385          1  ...         0\n",
              "798051          1  ...         0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Test Split"
      ],
      "metadata": {
        "id": "ZpeNsBn8x_3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training set and validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(train.text, train.Sentiment, random_state=0)\n",
        "#train_test_split(train['text'], train['Sentiment'], test_size=0.25, random_state=0)\n",
        "\n",
        "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n",
        "print('Show a review in the training set : \\n', X_train.iloc[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c41g9VMlyDPB",
        "outputId": "5b018d1e-67be-4ce4-8ee9-3acf26823f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load 78643 training examples and 26215 validation examples. \n",
            "\n",
            "Show a review in the training set : \n",
            " I have an old LP album of the same title as this Cd and with the same photo of Fats on the cover that I consider excellant. I thought this Cd was that album but its not- not even close. Very disapointing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "The following text preprocessing are implemented to convert raw reviews to cleaned review, so that it will be easier to do feature extraction in the next step.\n",
        "\n",
        "*   remove html tags using BeautifulSoup\n",
        "*   remove non-character such as digits and symbols\n",
        "*   convert to lower case\n",
        "*   remove stop words such as \"the\" and \"and\" if needed\n",
        "*   convert to root words by stemming if needed"
      ],
      "metadata": {
        "id": "hI5V_xY92whF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
        "             ):\n",
        "    '''\n",
        "    Convert a raw review to a cleaned review\n",
        "    '''\n",
        "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
        "    words = letters_only.lower().split() # convert to lower case \n",
        "    \n",
        "    if remove_stopwords: # remove stopword\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "        \n",
        "    if stemming==True: # stemming\n",
        "#         stemmer = PorterStemmer()\n",
        "        stemmer = SnowballStemmer('english') \n",
        "        words = [stemmer.stem(w) for w in words]\n",
        "        \n",
        "    if split_text==True:  # split text\n",
        "        return (words)\n",
        "    \n",
        "    return( \" \".join(words)) "
      ],
      "metadata": {
        "id": "vk0TKJ2t2rMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text data in training set and validation set\n",
        "X_train_cleaned = []\n",
        "X_test_cleaned = []\n",
        "\n",
        "for d in X_train:\n",
        "    X_train_cleaned.append(cleanText(d))\n",
        "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
        "    \n",
        "for d in X_test:\n",
        "    X_test_cleaned.append(cleanText(d))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sdWlmOs2ro-",
        "outputId": "0dcb4838-736a-40f7-b84a-0b80cf847800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Show a cleaned review in the training set : \n",
            " i have an old lp album of the same title as this cd and with the same photo of fats on the cover that i consider excellant i thought this cd was that album but its not not even close very disapointing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "\n",
        "Another common approach of word embedding is prediction based embedding, such as Word2Vec model. In gist, Word2Vec is a combination of two techniques: Continuous Bag of Words (CBoW) and skip-gram model. Both are shallow neural networks which learn weights for word vector representations.\n",
        "\n",
        "In this part, we will train Word2Vec model to create our own word vector representations using gensim library. Then we fit the feature vectors of the reviews to Random Forest Classifier. Here's the workflow of this part.\n",
        "\n",
        "* Parse review text to sentences (Word2Vec model takes a list of sentences as inputs)\n",
        "* Create volcabulary list using Word2Vec model\n",
        "* Transform each review into numerical representation by computing average feature vectors of words therein\n",
        "* Fit the average feature vectors to Random Forest Classifier\n",
        "\n"
      ],
      "metadata": {
        "id": "GKBED_Ig_9TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split review text into parsed sentences uisng NLTK's punkt tokenizer\n",
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def parseSent(review, tokenizer, remove_stopwords=False):\n",
        "    '''\n",
        "    Parse text into sentences\n",
        "    '''\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# Parse each review in the training set into sentences\n",
        "sentences = []\n",
        "for review in X_train_cleaned:\n",
        "    sentences += parseSent(review, tokenizer)\n",
        "    \n",
        "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
        "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gbCTLbr2rr_",
        "outputId": "f81aa200-15af-4b2e-afa3-f205c9002722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "78643 parsed sentence in the training set\n",
            "\n",
            "Show a parsed sentence in the training set : \n",
            " ['i', 'have', 'an', 'old', 'lp', 'album', 'of', 'the', 'same', 'title', 'as', 'this', 'cd', 'and', 'with', 'the', 'same', 'photo', 'of', 'fats', 'on', 'the', 'cover', 'that', 'i', 'consider', 'excellant', 'i', 'thought', 'this', 'cd', 'was', 'that', 'album', 'but', 'its', 'not', 'not', 'even', 'close', 'very', 'disapointing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit parsed sentences to Word2Vec model \n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
        "\n",
        "num_features = 300  #embedding dimension                     \n",
        "min_word_count = 10                \n",
        "num_workers = 4       \n",
        "context = 10                                                                                          \n",
        "downsampling = 1e-3 \n",
        "\n",
        "print(\"Training Word2Vec model ...\\n\")\n",
        "w2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,\\\n",
        "                 window = context, sample = downsampling)\n",
        "w2v.init_sims(replace=True)\n",
        "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n",
        "\n",
        "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) #4016 \n",
        "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wajfcpzFBhBB",
        "outputId": "f1acebd9-0212-4a8a-c00a-186eb5136d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Word2Vec model ...\n",
            "\n",
            "Number of words in the vocabulary list : 17456 \n",
            "\n",
            "Show first 10 words in the vocalbulary list  vocabulary list: \n",
            " ['the', 'i', 'and', 'a', 'to', 'it', 'of', 'this', 'is', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfrom the training data into feature vectors\n",
        "\n",
        "def makeFeatureVec(review, model, num_features):\n",
        "    '''\n",
        "    Transform a review to a feature vector by averaging feature vectors of words \n",
        "    appeared in that review and in the volcabulary list created\n",
        "    '''\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    nwords = 0.\n",
        "    index2word_set = set(model.wv.index2word) #index2word is the volcabulary list of the Word2Vec model\n",
        "    isZeroVec = True\n",
        "    for word in review:\n",
        "        if word in index2word_set: \n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec, model[word])\n",
        "            isZeroVec = False\n",
        "    if isZeroVec == False:\n",
        "        featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    '''\n",
        "    Transform all reviews to feature vectors using makeFeatureVec()\n",
        "    '''\n",
        "    counter = 0\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    for review in reviews:\n",
        "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
        "        counter = counter + 1\n",
        "    return reviewFeatureVecs"
      ],
      "metadata": {
        "id": "_e_8kYhHB96E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature vectors for training set\n",
        "X_train_cleaned = []\n",
        "for review in X_train:\n",
        "    X_train_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
        "trainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\n",
        "print(\"Training set : %d feature vectors with %d dimensions\" %trainVector.shape)\n",
        "\n",
        "\n",
        "# Get feature vectors for validation set\n",
        "X_test_cleaned = []\n",
        "for review in X_test:\n",
        "    X_test_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
        "testVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\n",
        "print(\"Validation set : %d feature vectors with %d dimensions\" %testVector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IZFUW_bCGpY",
        "outputId": "ae2cbd23-382f-4c94-93b0-ddd2068cb3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set : 78643 feature vectors with 300 dimensions\n",
            "Validation set : 26215 feature vectors with 300 dimensions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM\n",
        "Long Short Term Memory networks (LSTM) are a special kind of Recurrent Neural Networks (RNN), capable of learning long-term dependencies. LSTM can be very usefull in text mining problems since it involves dependencies in the sentences which can be caught in the \"memory\" of the LSTM.\n",
        "\n",
        "In this part, we train a simple LSTM and a LSTM with Word2Vec embedding to classify the reviews into positive and negative sentiment using Keras libarary.\n",
        "### Simple LSTM \n",
        "\n",
        "We need to preprocess the text data to 2D tensor before we fit into a simple LSTM. First, we tokenize the corpus by only considering top words (top_words = 20000), and transform reviews to numerical sequences using the trained tokenizer. Next, we make sure that all numerical sequences have the same length (maxlen=100) for modeling, by truncating long reviews and pad shorter reviews with zero values.\n",
        "\n",
        "To construct a simple LSTM, we use embedding class in Keras to construct the first layer. This embedding layer converts numerical sequence of words into a word embedding. We should note that the embedding class provides a convenient way to map discrete words into a continuous vector space, but it does not take the semantic similarity of the words into account. The next layer is the LSTM layer with 128 memory units. Finally, we use a dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (positive sentiment and negative sentiment). Since it is a binary classification problem, log loss is used as the loss function (binary_crossentropy in Keras). ADAM optimization algorithm is used.\n",
        "Here's the workflow in this part.\n",
        "\n",
        "* Prepare X_train and X_test to 2D tensor\n",
        "* Train a simple LSTM (embeddign layer => LSTM layer => dense layer)\n",
        "* Compile and fit the model using log loss function and ADAM optimizer"
      ],
      "metadata": {
        "id": "DuGc3MQ4C0wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_words = 20000 \n",
        "maxlen = 100 \n",
        "batch_size = 32\n",
        "nb_classes = 2\n",
        "nb_epoch = 3\n",
        "\n",
        "\n",
        "# Vectorize X_train and X_test to 2D tensor\n",
        "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
        "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "# one-hot encoding of y_train and y_test\n",
        "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
        "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "print('X_train shape:', X_train_seq.shape)\n",
        "print('X_test shape:', X_test_seq.shape)\n",
        "print('y_train shape:', y_train_seq.shape)\n",
        "print('y_test shape:', y_test_seq.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_oYcdNPC3Tw",
        "outputId": "b2e9e2cf-afc2-4c52-8f1c-433f5caa0290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (78643, 100)\n",
            "X_test shape: (26215, 100)\n",
            "y_train shape: (78643, 2)\n",
            "y_test shape: (26215, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a simple LSTM\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(top_words, 128, dropout=0.2))\n",
        "model1.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
        "model1.add(Dense(nb_classes))\n",
        "model1.add(Activation('softmax'))\n",
        "model1.summary()\n",
        "\n",
        "# Compile LSTM\n",
        "model1.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
        "\n",
        "# Model evluation\n",
        "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
        "print('Test loss : {:.4f}'.format(score[0]))\n",
        "print('Test accuracy : {:.4f}'.format(score[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "TOv2ekZrD3BW",
        "outputId": "64effe71-2355-4531-e8de-98fe86139ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f233ce6c1727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Construct a simple LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/embeddings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# to a loss of precision.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'autocast'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     }\n\u001b[1;32m    331\u001b[0m     \u001b[0;31m# Validate optional keyword arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# Mutable properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m   1168\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'dropout')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get weight matrix of the embedding layer\n",
        "model1.layers[0].get_weights()[0] # weight matrix of the embedding layer, word-by-dim matrix\n",
        "print(\"Size of weight matrix in the embedding layer : \", \\\n",
        "      model1.layers[0].get_weights()[0].shape)\n",
        "\n",
        "# get weight matrix of the hidden layer\n",
        "print(\"Size of weight matrix in the hidden layer : \", \\\n",
        "      model1.layers[1].get_weights()[0].shape) # weight dim of LSTM - w\n",
        "\n",
        "# get weight matrix of the output layer\n",
        "print(\"Size of weight matrix in the output layer : \", \\\n",
        "      model1.layers[2].get_weights()[0].shape) # weight dim of dense layer"
      ],
      "metadata": {
        "id": "wnUCjXq7ESMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM with Word2Vec Embedding\n",
        "\n",
        "In the simple LSTM model constructed above, the embedding class in Keras comes in handy to converts numerical sequence of words into a word embedding, but it does not take the semantic similarity of the words into account. The model assigns random weights to the embedding layer and learn the embeddings by minimizing the global error of the network.\n",
        "\n",
        "Instead of using random weights, we can use pretrained word embeddings to initialize the weight of an embedding layer. In this part, we use the Word2Vec embedding trained in Part 4 to intialize the weights of embedding layer in LSTM.\n",
        "\n",
        "* Load pretrained word embedding model\n",
        "* Construct embedding layer using embedding matrix as weights\n",
        "* Train a LSTM with Word2Vec embedding (embeddign layer => LSTM layer => dense layer)\n",
        "* Compile and fit the model using log loss function and ADAM optimizer"
      ],
      "metadata": {
        "id": "fHAG7i8KETj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained Word2Vec model\n",
        "w2v = Word2Vec.load(\"w2v_300features_10minwordcounts_10context\")\n",
        "\n",
        "\n",
        "# Get Word2Vec embedding matrix\n",
        "embedding_matrix = w2v.wv.syn0  # embedding matrix, type = numpy.ndarray \n",
        "print(\"Shape of embedding matrix : \", embedding_matrix.shape) #(volcabulary size, embedding dimension)\n",
        "# w2v.wv.syn0[0] #feature vector of the first word in the volcabulary list"
      ],
      "metadata": {
        "id": "gu_3ixyVEfGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63590112-9e09-4f0a-8e4f-c1b5b4beb644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embedding matrix :  (17456, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_words = embedding_matrix.shape[0] #4016\n",
        "maxlen = 100 \n",
        "batch_size = 32\n",
        "nb_classes = 2\n",
        "nb_epoch = 3\n",
        "\n",
        "\n",
        "# Vectorize X_train and X_test to 2D tensor\n",
        "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
        "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "# one-hot encoding of y_train and y_test\n",
        "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
        "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "print('X_train shape:', X_train_seq.shape)\n",
        "print('X_test shape:', X_test_seq.shape)\n",
        "print('y_train shape:', y_train_seq.shape)\n",
        "print('y_test shape:', y_test_seq.shape)"
      ],
      "metadata": {
        "id": "c_gIljpGEkid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7022d94d-eb7f-457d-ed8c-e4cfce46ba5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/text.py:180: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (78643, 100)\n",
            "X_test shape: (26215, 100)\n",
            "y_train shape: (78643, 2)\n",
            "y_test shape: (26215, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct Word2Vec embedding layer\n",
        "embedding_layer = Embedding(embedding_matrix.shape[0], #4016\n",
        "                            embedding_matrix.shape[1], #300\n",
        "                            weights=[embedding_matrix])\n",
        "\n",
        "\n",
        "# Construct LSTM with Word2Vec embedding\n",
        "model2 = Sequential()\n",
        "model2.add(embedding_layer)\n",
        "model2.add(LSTM(units=128)) \n",
        "model2.add(Dense(nb_classes))\n",
        "model2.add(Activation('softmax'))\n",
        "model2.summary()\n",
        "\n",
        "# Compile model\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model2.fit(X_train_seq, y_train_seq, batch_size=batch_size, epochs=3, verbose=1)\n",
        "\n",
        "\n",
        "# Model evaluation\n",
        "score = model2.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
        "print('Test loss : {:.4f}'.format(score[0]))\n",
        "print('Test accuracy : {:.4f}'.format(score[1]))"
      ],
      "metadata": {
        "id": "w08J8YSeEw--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153ef078-645b-4830-e567-ccf524d5fd8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, None, 300)         5236800   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 128)               219648    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,456,706\n",
            "Trainable params: 5,456,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "2458/2458 [==============================] - 837s 339ms/step - loss: 0.3560 - accuracy: 0.8448\n",
            "Epoch 2/3\n",
            "2458/2458 [==============================] - 834s 339ms/step - loss: 0.2146 - accuracy: 0.9164\n",
            "Epoch 3/3\n",
            "2458/2458 [==============================] - 816s 332ms/step - loss: 0.1365 - accuracy: 0.9496\n",
            "820/820 [==============================] - 36s 44ms/step - loss: 0.3262 - accuracy: 0.8864\n",
            "Test loss : 0.3262\n",
            "Test accuracy : 0.8864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get weight matrix of the embedding layer\n",
        "print(\"Size of weight matrix in the embedding layer : \", \\\n",
        "      model2.layers[0].get_weights()[0].shape)\n",
        "\n",
        "# get weight matrix of the hidden layer\n",
        "print(\"Size of weight matrix in the hidden layer : \", \\\n",
        "      model2.layers[1].get_weights()[0].shape) # weight dim of LSTM - w\n",
        "\n",
        "# get weight matrix of the output layer\n",
        "print(\"Size of weight matrix in the output layer : \", \\\n",
        "      model2.layers[2].get_weights()[0].shape) # weight dim of dense layer"
      ],
      "metadata": {
        "id": "l0nYFyN1E0iP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2e5638-757b-498e-8576-771e09abaf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of weight matrix in the embedding layer :  (17456, 300)\n",
            "Size of weight matrix in the hidden layer :  (300, 512)\n",
            "Size of weight matrix in the output layer :  (128, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2NtEVyxkEojJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}